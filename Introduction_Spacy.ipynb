{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38dc1594-b889-45b0-a46b-de951e982c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U pip setuptools wheel\n",
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22a89d8-893e-48bb-84dd-2cb302ad00fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN dobj\n",
      "startup NOUN ccomp\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb67dcb-9049-4bc4-8b80-83f2041d70e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "input = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(input)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13326524-df80-4a9a-b52b-5b15c0897932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text==input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7b6d81-48e5-4361-bdc5-0fc24bebcb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gimme', 'that']\n",
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"gimme that\")  # phrase to tokenize\n",
    "print([w.text for w in doc])  # ['gimme', 'that']\n",
    "\n",
    "# Add special case rule\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "# Check new tokenization\n",
    "print([w.text for w in nlp(\"gimme that\")])  # ['gim', 'me', 'that']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b07b0-96d8-4fa6-afec-1b4459e497a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomTokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cc95e26-e8ac-4264-8862-d4ec4121b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "748fa9bb-5325-4f28-9761-36c8a5068a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gimme that"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"gimme that\"\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29810e95-1475-44c6-a8ed-6c3902c48ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gim', 'me', 'that']\n"
     ]
    }
   ],
   "source": [
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "\n",
    "text = \"gimme that\"\n",
    "\n",
    "print([w.text for w in nlp(text)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c5c58-edeb-43e0-9986-f1d86826143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e89de19-1650-4b0d-a903-fac5df34d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN dep xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "851bdb7c-d818-416a-859c-476300f21b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cbdc6f9-9f40-4a1a-8c23-bf52244437d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d0ab1-b9c0-430b-b0b8-ee6fcf6290ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22565b11-f075-4810-9d3f-139f390c96c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before []\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"fb is hiring a new vice president of global policy\")\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('Before', ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5e3219f-5de9-4694-9dad-f9f1b9ea0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ents = list(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0108f69a-c240-47de-921c-11f3101c6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a span for the new entity\n",
    "fb_ent = Span(doc, 0, 1, label=\"ORG\")\n",
    "orig_ents = list(doc.ents)\n",
    "\n",
    "# Option 1: Modify the provided entity spans, leaving the rest unmodified\n",
    "doc.set_ents([fb_ent], default=\"unmodified\")\n",
    "\n",
    "# Option 2: Assign a complete list of ents to doc.ents\n",
    "doc.ents = orig_ents + [fb_ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fe24b9f-426f-4655-9810-f6d32137d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after [('fb', 0, 2, 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print('after', ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bb1bd57-5751-4e28-9ed0-34e74b51068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb 0 2 ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850b234-8544-45ae-98ea-9bb191df5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of Vocabulory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0805352-9009-42fb-a6cb-1f57fa581a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "banana True 31.620354 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fd28131-39fc-48a0-882a-6a326f54314c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.6871286202797843\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")  # make sure to use larger package!\n",
    "\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e4e3641-cd64-447e-9db6-37bbc87012a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salty fries <-> hamburgers 0.6901010870933533\n"
     ]
    }
   ],
   "source": [
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ee5360c-4e42-4027-a9fb-5e4a822c6ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "banana True 31.620354 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "tokens = nlp(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17bd97e8-b754-4b55-810e-c59fe6dbaf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31b92334-16cc-4ee6-9e17-558fedc51759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect=Prog|Tense=Pres|VerbForm=Part\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I was reading the paper.\")\n",
    "token = doc[2]  # 'I'\n",
    "print(token.morph)  # 'Case=Nom|Number=Sing|Person=1|PronType=Prs'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdab2345-1eff-4459-b963-8b011da7bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(token.morph.get(\"PronType\"))  # ['Prs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83a94a58-d52b-4835-af63-72db91abab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case=Nom|Person=2|PronType=Prs\n",
      "PRON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Where are you?\")\n",
    "print(doc[2].morph)  # 'Case=Nom|Person=2|PronType=Prs'\n",
    "print(doc[2].pos_)  # 'PRON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d67c106-c849-4a19-aae5-a42b3f74457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule\n",
      "['I', 'be', 'read', 'the', 'paper', '.']\n"
     ]
    }
   ],
   "source": [
    "# English pipelines include a rule-based lemmatizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "print(lemmatizer.mode)  # 'rule'\n",
    "\n",
    "doc = nlp(\"I was reading the paper.\")\n",
    "print([token.lemma_ for token in doc])\n",
    "# ['I', 'be', 'read', 'the', 'paper', '.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df8709d9-1c08-4c38-bcbf-d7ed109d21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Collecting all the emails from text Documnent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b654c55-790b-4068-8888-3cc41d9c7c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day   \\temail\\n',\n",
       " '-----\\t------------\\t------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
       " 'Joe      1 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D:\\Data\\Daval\\students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2ee71ad-a884-4fce-a02a-9e75c7ae57fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9da8fbdb-fe36-4ecc-9ac4-c55a21f2bd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff509974-6409-453b-a6c2-1db3c3c0f503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
